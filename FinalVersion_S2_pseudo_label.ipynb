{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "FinalVersion_S2_pseudo_label.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoclementliao/bert-cls-notebook/blob/master/FinalVersion_S2_pseudo_label.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXrAlQ6EmeEZ",
        "colab_type": "code",
        "outputId": "12759786-49e7-4a46-ac19-d8972d81c17f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 26 08:25:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0ZbLarhm7B4",
        "colab_type": "code",
        "outputId": "7ddb7f1e-3b7a-4637-c581-11a41d5960ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzBUkvEf8Rtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "version_name = 'FV_Bert'\n",
        "test_name = f'/content/drive/My Drive/Colab Notebooks/kaggle_covid19NLP/submission/test_sub{version_name}.csv'\n",
        "lr_global = 1e-6\n",
        "lr_dense = 2e-4\n",
        "batch_size = 80\n",
        "epochs_num = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2KuGVXdnGOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "806cd6e3-54a4-46e6-fde0-fa6cab3ea4ed"
      },
      "source": [
        "% cp -r /content/drive/My\\ Drive/Colab\\ Notebooks/kaggle_covid19NLP /content\n",
        "% cd /content/kaggle_covid19NLP/notebook\n",
        "# ! unzip /content/kaggle_covid19NLP/RoBERTa_zh_L12_PyTorch.zip -d /content/kaggle_covid19NLP/RoBERTa_zh_L12_PyTorch\n",
        "# ! unzip /content/kaggle_covid19NLP/data/test_dataset.zip -d /content/kaggle_covid19NLP/data/test_dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/kaggle_covid19NLP/notebook\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LGHV67EjITC",
        "colab_type": "text"
      },
      "source": [
        "# Install transformers library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb4QbS4orR0z",
        "colab_type": "code",
        "outputId": "52f99cc8-ebb3-4ea1-d525-7b6cdd451793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 30.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 59.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.43)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 53.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.43)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=830ae6ff077d127528d6fe801dd34fd9e8d84b5fbaaffb53cc0eebc04c94d0fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK7e9Ettq2Sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_probability as tfp\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM8LIWwJjITE",
        "colab_type": "code",
        "outputId": "108e2dc1-8bfc-4f4a-e8ff-d052bacf8fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "# import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# from tensorflow.keras.layers import (GlobalAveragePooling1D, Reshape, Conv1D,\n",
        "#                                      multiply)\n",
        "import os\n",
        "import time, gc\n",
        "\n",
        "from transformers import *\n",
        "print(tf.__version__)\n",
        "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va7uR2_4Dzzl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "3279858f-baf8-4f6d-a55f-64340652d625"
      },
      "source": [
        "UNLABEL_PATH = '../data/train_dataset/nCoV_900k_train.unlabled.csv' \n",
        "SUB_PATH = '../data/submit_example.csv'\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 140\n",
        "input_categories = '微博中文内容'\n",
        "output_categories = '情感倾向'\n",
        "\n",
        "df_test = pd.read_csv(UNLABEL_PATH)\n",
        "df_sub = pd.read_csv(SUB_PATH)\n",
        "\n",
        "print('unlabeled data shape =', df_test.shape)\n",
        "df_test.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unlabeled data shape = (900000, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>微博id</th>\n",
              "      <th>微博发布时间</th>\n",
              "      <th>发布人账号</th>\n",
              "      <th>微博中文内容</th>\n",
              "      <th>微博图片</th>\n",
              "      <th>微博视频</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4456074364642450</td>\n",
              "      <td>01月01日 23:59</td>\n",
              "      <td>Dear_Christin</td>\n",
              "      <td>2020年第一天，宝宝晚上发烧了。上个月的肺炎经历还历历在目，不觉又开始揪心。目前低烧安稳睡...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4456069114155340</td>\n",
              "      <td>01月01日 23:38</td>\n",
              "      <td>Maggie大美Y</td>\n",
              "      <td>生病发烧一直就没好～就当预示着2020年是个风风火火的好好好运年～新年第一天有肉吃，以后天天...</td>\n",
              "      <td>['https://ww1.sinaimg.cn/orj360/006TMJgaly1gah...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4456068145061890</td>\n",
              "      <td>01月01日 23:34</td>\n",
              "      <td>折眉几年</td>\n",
              "      <td>害新年第一天就发烧????</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4456064403586410</td>\n",
              "      <td>01月01日 23:20</td>\n",
              "      <td>秃头少女Brushblog</td>\n",
              "      <td>是心动啊，糟糕眼神躲不掉。对你莫名的心跳，竟然停不了对你的迷恋感觉要发烧。@namjoohy...</td>\n",
              "      <td>['https://wx3.sinaimg.cn/orj480/63becc16gy1g7v...</td>\n",
              "      <td>['https://f.video.weibocdn.com/0009osQVlx07xJg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4456060284446840</td>\n",
              "      <td>01月01日 23:03</td>\n",
              "      <td>Akemi庄蝶</td>\n",
              "      <td>过去的一整年有好好喜欢你。不过今年不可以啦，毕竟，可不能连自己都不要了。跨年给你打电话，听到...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               微博id  ...                                               微博视频\n",
              "0  4456074364642450  ...                                                 []\n",
              "1  4456069114155340  ...                                                 []\n",
              "2  4456068145061890  ...                                                 []\n",
              "3  4456064403586410  ...  ['https://f.video.weibocdn.com/0009osQVlx07xJg...\n",
              "4  4456060284446840  ...                                                 []\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR6sHlQTjITO",
        "colab_type": "text"
      },
      "source": [
        "# Convert tokens to input_ids\n",
        "<img src=\"https://imgkr.cn-bj.ufileos.com/341d1c83-45cf-4e9b-a656-a547cd0f2c67.png\" width=\"50%\" height=\"50%\" />\n",
        "\n",
        "transformer chinese weight\n",
        "- https://github.com/ymcui/Chinese-BERT-wwm\n",
        "- https://github.com/brightmart/roberta_zh\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27cLzFjUjITQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _convert_to_transformer_inputs(instance, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
        "    \"\"\" return input_ids,token_type_ids,attention_mask\"\"\"\n",
        "    inputs = tokenizer.encode_plus(instance,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_sequence_length,\n",
        "        truncation_strategy= 'longest_first')\n",
        "\n",
        "    input_ids =  inputs[\"input_ids\"]\n",
        "    input_masks = inputs[\"attention_mask\"]\n",
        "    input_segments = inputs[\"token_type_ids\"]\n",
        "    padding_length = max_sequence_length - len(input_ids)\n",
        "    #padding\n",
        "    padding_id = tokenizer.pad_token_id\n",
        "    input_ids = input_ids + ([padding_id] * padding_length)\n",
        "    input_masks = input_masks + ([0] * padding_length)\n",
        "    input_segments = input_segments + ([0] * padding_length)\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "\n",
        "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for instance in tqdm(df[columns]):\n",
        "        \n",
        "        ids, masks, segments = _convert_to_transformer_inputs(str(instance), tokenizer, max_sequence_length)\n",
        "        \n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "\n",
        "    return [np.asarray(input_ids, dtype=np.int32), \n",
        "            np.asarray(input_masks, dtype=np.int32), \n",
        "            np.asarray(input_segments, dtype=np.int32)\n",
        "           ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ikbnW8cjITU",
        "colab_type": "code",
        "outputId": "8300465e-7e00-4ba7-805c-a3750612d6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "BERT_PATH = '/content/kaggle_covid19NLP/bert_base_chinese'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'/bert-base-chinese-vocab.txt')\n",
        "\n",
        "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
            "100%|██████████| 900000/900000 [08:38<00:00, 1736.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0JJcTz7jITo",
        "colab_type": "text"
      },
      "source": [
        "# BERT model\n",
        "\n",
        "https://huggingface.co/models\n",
        "\n",
        "\n",
        "<img src=\"https://imgkr.cn-bj.ufileos.com/9115ac01-f455-498b-8c38-9c4abb04046c.png\" width=\"50%\" height=\"50%\" />\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWVLFKE6jITp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yNyhbwfjITz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "class Initi(tf.keras.initializers.Initializer ):\n",
        "\n",
        "    \"\"\"Initializer that generates tensors initialized to 1.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    def __call__(self, shape, dtype=None):\n",
        "\n",
        "        out = np.zeros( shape=shape, dtype=np.float32)\n",
        "        #out[0] = 1\n",
        "        #out[-1] = -1\n",
        "        #out[-2] = 0\n",
        "        #out[-3] = 0\n",
        "        #out[-4] = 0\n",
        "        return out\n",
        "\n",
        "\n",
        "class Weighted_sum(Layer):\n",
        "\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        super(Weighted_sum, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.kernel = self.add_weight(name='weighted_sum', \n",
        "                                      shape=(input_shape[-1], self.output_dim),\n",
        "                                      initializer=Initi(),\n",
        "                                      trainable=True)\n",
        "        super(Weighted_sum, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x):\n",
        "        return K.dot(x, K.softmax(self.kernel,-2))#tf.linalg.matmul\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape#(input_shape[0], self.output_dim)\n",
        "\n",
        "\n",
        "def SE_layer(stacked_layers):\n",
        "    num_filters_out = stacked_layers.shape[2]\n",
        "    se = tf.keras.layers.GlobalAveragePooling1D()(stacked_layers)\n",
        "    se = tf.keras.layers.Reshape((1, num_filters_out))(se)\n",
        "    se = tf.keras.layers.Conv1D(num_filters_out//4, 1,\n",
        "                padding='same',\n",
        "                activation=\"relu\",\n",
        "                kernel_initializer='he_normal',\n",
        "                name='SE_layer1')(se)\n",
        "    se = tf.keras.layers.Conv1D(num_filters_out, 1,\n",
        "                padding='same',\n",
        "                activation='sigmoid',\n",
        "                kernel_initializer='he_normal',\n",
        "                name='SE_layer2')(se)\n",
        "\n",
        "    Layers_SE = tf.keras.layers.multiply([stacked_layers, se])\n",
        "    return Layers_SE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTBe874RjIT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tensorflow.keras.legacy import interfaces\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.python.keras.optimizers import Optimizer\n",
        "\n",
        "class Adam_lr_mult(Optimizer):\n",
        "    \"\"\"Adam optimizer.\n",
        "    Adam optimizer, with learning rate multipliers built on Keras implementation\n",
        "    # Arguments\n",
        "        lr: float >= 0. Learning rate.\n",
        "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
        "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
        "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
        "        decay: float >= 0. Learning rate decay over each update.\n",
        "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
        "            algorithm from the paper \"On the Convergence of Adam and\n",
        "            Beyond\".\n",
        "    # References\n",
        "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
        "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
        "        \n",
        "    AUTHOR: Erik Brorson\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001,name = 'mAdam', beta_1=0.9, beta_2=0.999,\n",
        "                 epsilon=None, decay=0., amsgrad=False,\n",
        "                 multipliers=None, debug_verbose=False,**kwargs):\n",
        "        super(Adam_lr_mult, self).__init__(**kwargs)\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
        "            self.lr = K.variable(lr, name='lr')\n",
        "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
        "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
        "            self.decay = K.variable(decay, name='decay')\n",
        "        if epsilon is None:\n",
        "            epsilon = K.epsilon()\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_decay = decay\n",
        "        self.amsgrad = amsgrad\n",
        "        self.multipliers = multipliers\n",
        "        self.debug_verbose = debug_verbose\n",
        "\n",
        "    #@interfaces.legacy_get_updates_support\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
        "                                                  K.dtype(self.decay))))\n",
        "\n",
        "        t = K.cast(self.iterations, K.floatx()) + 1\n",
        "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
        "                     (1. - K.pow(self.beta_1, t)))\n",
        "\n",
        "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        if self.amsgrad:\n",
        "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        else:\n",
        "            vhats = [K.zeros(1) for _ in params]\n",
        "        self.weights = [self.iterations] + ms + vs + vhats\n",
        "\n",
        "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
        "\n",
        "            # Learning rate multipliers\n",
        "            if self.multipliers:\n",
        "                multiplier = [mult for mult in self.multipliers if mult in p.name]\n",
        "            else:\n",
        "                multiplier = None\n",
        "            if multiplier:\n",
        "                new_lr_t = lr_t * self.multipliers[multiplier[0]]\n",
        "                if self.debug_verbose:\n",
        "                    print('Setting {} to learning rate {}'.format(multiplier[0], new_lr_t))\n",
        "                    print(K.get_value(new_lr_t))\n",
        "            else:\n",
        "                new_lr_t = lr_t\n",
        "                if self.debug_verbose:\n",
        "                    print('No change in learning rate {}'.format(p.name))\n",
        "                    print(K.get_value(new_lr_t))\n",
        "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
        "            if self.amsgrad:\n",
        "                vhat_t = K.maximum(vhat, v_t)\n",
        "                p_t = p - new_lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
        "                self.updates.append(K.update(vhat, vhat_t))\n",
        "            else:\n",
        "                p_t = p - new_lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
        "\n",
        "            self.updates.append(K.update(m, m_t))\n",
        "            self.updates.append(K.update(v, v_t))\n",
        "            new_p = p_t\n",
        "\n",
        "            # Apply constraints.\n",
        "            if getattr(p, 'constraint', None) is not None:\n",
        "                new_p = p.constraint(new_p)\n",
        "\n",
        "            self.updates.append(K.update(p, new_p))\n",
        "        return self.updates\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'lr': float(K.get_value(self.lr)),\n",
        "                  'beta_1': float(K.get_value(self.beta_1)),\n",
        "                  'beta_2': float(K.get_value(self.beta_2)),\n",
        "                  'decay': float(K.get_value(self.decay)),\n",
        "                  'epsilon': self.epsilon,\n",
        "                  'amsgrad': self.amsgrad,\n",
        "                  'multipliers':self.multipliers}\n",
        "        base_config = super(Adam_lr_mult, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVT5CVYNjIT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate_multipliers = {}\n",
        "learning_rate_multipliers['weighted_sum'] = lr_dense/lr_global\n",
        "learning_rate_multipliers['dense0'] = lr_dense/lr_global\n",
        "learning_rate_multipliers['dense1'] = lr_dense/lr_global\n",
        "learning_rate_multipliers['dense2'] = lr_dense/lr_global\n",
        "#learning_rate_multipliers['layer_3'] = 0.1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBQ3cX8QjIUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r3RwyiAjIUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    input_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)    \n",
        "    input_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)    \n",
        "    input_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)    \n",
        "    config = BertConfig.from_pretrained( '../bert_base_chinese/bert-base-chinese-config.json', output_hidden_states=True)\n",
        "    bert_model = TFBertModel.from_pretrained('../bert_base_chinese/bert-base-chinese-tf_model.h5', config=config)\n",
        "    # config = BertConfig.from_pretrained( '../RoBERTa_zh_L12_PyTorch/config.json', output_hidden_states=True)\n",
        "    # bert_model = TFBertModel.from_pretrained('../RoBERTa_zh_L12_PyTorch/pytorch_model.bin',from_pt=True, config=config)\n",
        "    \n",
        "    sequence_output, pooler_output, hidden_states = bert_model(input_id, attention_mask=input_mask, token_type_ids=input_atn)\n",
        "    print(sequence_output.shape)\n",
        "    print(len(hidden_states))\n",
        "    #print(hidden_states[-1].shape)\n",
        "    #(bs,140,768)(bs,768)\n",
        "    \n",
        "    stacked_layers = tf.stack([tf.keras.layers.Dropout(0.1)(layer[:, 0, :]) for layer in hidden_states[1:]],axis = -1)#,noise_shape=(None,None,1)\n",
        "    \n",
        "    #print(shape)\n",
        "    # stacked_layers = SE_layer(stacked_layers)\n",
        "    cls_output = Weighted_sum(1)(stacked_layers)\n",
        "    cls_output = K.squeeze(cls_output,axis = -1)\n",
        "    print(cls_output.shape)\n",
        "    #cls_output = tf.keras.layers.GlobalAveragePooling1D()(cls_output)\n",
        "    #x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    #x = tf.keras.layers.Dense(3, activation='softmax',name = \"out\")(x)\n",
        "    # multisample dropout (wut): https://arxiv.org/abs/1905.09788\n",
        "    dense_out = []\n",
        "    FC0  = tf.keras.layers.Dense(128, activation='relu',name = \"dense0\")\n",
        "    FC1 = tf.keras.layers.Dense(128, activation='relu',name = \"dense1\")\n",
        "    FC2  = tf.keras.layers.Dense(3, activation='softmax',name = \"dense2\")\n",
        "    for _ in range(8):\n",
        "        x = tf.keras.layers.Dropout(0.15)(cls_output)\n",
        "        \n",
        "        x = FC0(x)\n",
        "        x = tf.keras.layers.Dropout(0.15)(x)\n",
        "        \n",
        "        #x = FC1(x)\n",
        "        #x = tf.keras.layers.Dropout(0.2)(x)\n",
        "        x = FC2(x)\n",
        "        dense_out.append(x)\n",
        "    \n",
        "    out = tf.keras.layers.average(dense_out)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_id, input_mask, input_atn], outputs=out)\n",
        "    #layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
        "    bert_model.bert.pooler.dense.trainable  = False\n",
        "    #optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    opt = Adam_lr_mult(lr = lr_global, multipliers=learning_rate_multipliers)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc', f1])\n",
        "\n",
        "    # FL=lambda y_true,y_pred: Focal_Loss(y_true, y_pred, alpha=0.25, gamma=2)\n",
        "    # model.compile(loss=FL, optimizer=opt, metrics=['acc', f1])   \n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "SvxMzcRKjIUK",
        "colab_type": "code",
        "outputId": "dfb94b9f-0e10-4994-eddc-2a5aac39e4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = create_model()\n",
        "# model.save_weights(\"init_weights.h5\")\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "(None, 140, 768)\n",
            "13\n",
            "(None, 768)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 140)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 140)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 140)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model (TFBertModel)     ((None, 140, 768), ( 102267648   input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 768)]        0           tf_bert_model[0][3]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [(None, 768)]        0           tf_bert_model[0][4]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [(None, 768)]        0           tf_bert_model[0][5]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te [(None, 768)]        0           tf_bert_model[0][6]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_4 (Te [(None, 768)]        0           tf_bert_model[0][7]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_5 (Te [(None, 768)]        0           tf_bert_model[0][8]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_6 (Te [(None, 768)]        0           tf_bert_model[0][9]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_7 (Te [(None, 768)]        0           tf_bert_model[0][10]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_8 (Te [(None, 768)]        0           tf_bert_model[0][11]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_9 (Te [(None, 768)]        0           tf_bert_model[0][12]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_10 (T [(None, 768)]        0           tf_bert_model[0][13]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_11 (T [(None, 768)]        0           tf_bert_model[0][14]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_4[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_6[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_7[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_8[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_9[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_10[0][0\n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice_11[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_stack (TensorFlowOp [(None, 768, 12)]    0           dropout_37[0][0]                 \n",
            "                                                                 dropout_38[0][0]                 \n",
            "                                                                 dropout_39[0][0]                 \n",
            "                                                                 dropout_40[0][0]                 \n",
            "                                                                 dropout_41[0][0]                 \n",
            "                                                                 dropout_42[0][0]                 \n",
            "                                                                 dropout_43[0][0]                 \n",
            "                                                                 dropout_44[0][0]                 \n",
            "                                                                 dropout_45[0][0]                 \n",
            "                                                                 dropout_46[0][0]                 \n",
            "                                                                 dropout_47[0][0]                 \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "weighted_sum (Weighted_sum)     (None, 768, 1)       12          tf_op_layer_stack[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze (TensorFlow [(None, 768)]        0           weighted_sum[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 768)          0           tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 768)          0           tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 768)          0           tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 768)          0           tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 768)          0           tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 768)          0           tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 768)          0           tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 768)          0           tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense0 (Dense)                  (None, 128)          98432       dropout_49[0][0]                 \n",
            "                                                                 dropout_51[0][0]                 \n",
            "                                                                 dropout_53[0][0]                 \n",
            "                                                                 dropout_55[0][0]                 \n",
            "                                                                 dropout_57[0][0]                 \n",
            "                                                                 dropout_59[0][0]                 \n",
            "                                                                 dropout_61[0][0]                 \n",
            "                                                                 dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 128)          0           dense0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 128)          0           dense0[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 128)          0           dense0[2][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 128)          0           dense0[3][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 128)          0           dense0[4][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 128)          0           dense0[5][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 128)          0           dense0[6][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 128)          0           dense0[7][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense2 (Dense)                  (None, 3)            387         dropout_50[0][0]                 \n",
            "                                                                 dropout_52[0][0]                 \n",
            "                                                                 dropout_54[0][0]                 \n",
            "                                                                 dropout_56[0][0]                 \n",
            "                                                                 dropout_58[0][0]                 \n",
            "                                                                 dropout_60[0][0]                 \n",
            "                                                                 dropout_62[0][0]                 \n",
            "                                                                 dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average (Average)               (None, 3)            0           dense2[0][0]                     \n",
            "                                                                 dense2[1][0]                     \n",
            "                                                                 dense2[2][0]                     \n",
            "                                                                 dense2[3][0]                     \n",
            "                                                                 dense2[4][0]                     \n",
            "                                                                 dense2[5][0]                     \n",
            "                                                                 dense2[6][0]                     \n",
            "                                                                 dense2[7][0]                     \n",
            "==================================================================================================\n",
            "Total params: 102,366,479\n",
            "Trainable params: 101,775,887\n",
            "Non-trainable params: 590,592\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZm-q0FajIU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import glob\n",
        "# model_list = glob.glob('/content/kaggle_covid19NLP/model/FV_*.h5')\n",
        "model_list=['/content/kaggle_covid19NLP/model/bert_drop-0.h5',\n",
        "            '/content/kaggle_covid19NLP/model/bert_drop-1.h5',\n",
        "            '/content/kaggle_covid19NLP/model/bert_drop-2.h5',\n",
        "            '/content/kaggle_covid19NLP/model/bert_drop-3.h5',\n",
        "            '/content/kaggle_covid19NLP/model/bert_drop-4.h5']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIbrJv1Zt5i_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "235b9217-1548-42bb-aeb7-debaa51f94c2"
      },
      "source": [
        "from datetime import datetime\n",
        "print(\"now =\", datetime.now() )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "now = 2020-04-26 08:38:14.641143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pad8YUbiqN-z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "f77f8940-29df-4fec-9f1f-d518ac8e36a5"
      },
      "source": [
        "df_pdata = pd.DataFrame(columns=['微博id', input_categories, '-1', '0', '1'])\n",
        "df_pdata['微博id'] = df_test['微博id']\n",
        "df_pdata[input_categories] = df_test[input_categories]\n",
        "\n",
        "test_preds = []\n",
        "start_time = time.time()\n",
        "for i, model_path in enumerate(model_list):\n",
        "    model.load_weights(model_path)\n",
        "    print(model_path)\n",
        "    test = model.predict(test_inputs, batch_size = 2048)\n",
        "    df_pdata[['-1', '0', '1']] = test\n",
        "\n",
        "    df_pdata.to_csv(f'/content/drive/My Drive/Colab Notebooks/kaggle_covid19NLP/data/train_dataset/nCoV_900k_train.Plabled{model_path[-4]}.csv', index=False)\n",
        "    test_preds.append(test)\n",
        "    \n",
        "    print(time.time()-start_time)\n",
        "    start_time = time.time()\n",
        "\n",
        "pseudo_label = np.average(test_preds, axis=0)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/kaggle_covid19NLP/model/bert_drop-0.h5\n",
            "4198.5248103141785\n",
            "/content/kaggle_covid19NLP/model/bert_drop-1.h5\n",
            "4187.188954114914\n",
            "/content/kaggle_covid19NLP/model/bert_drop-2.h5\n",
            "4187.199415683746\n",
            "/content/kaggle_covid19NLP/model/bert_drop-3.h5\n",
            "4187.453500509262\n",
            "/content/kaggle_covid19NLP/model/bert_drop-4.h5\n",
            "4187.298005104065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrqORkh-qOFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pseudo_label = np.average(test_preds, axis=0)\n",
        "df_pdata[['-1', '0', '1']] = pseudo_label\n",
        "df_pdata.to_csv('/content/drive/My Drive/Colab Notebooks/kaggle_covid19NLP/data/train_dataset/nCoV_900k_train.Plabled.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SyH6t2eu16x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "7711b632-4e3d-404b-8370-0410f56e9505"
      },
      "source": [
        "df_good = df_pdata[(df_pdata['-1']>0.6) | (df_pdata['0']>0.6) | (df_pdata['1']>0.6)]\n",
        "df_good.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>微博id</th>\n",
              "      <th>微博中文内容</th>\n",
              "      <th>-1</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4456068145061890</td>\n",
              "      <td>害新年第一天就发烧????</td>\n",
              "      <td>0.945920</td>\n",
              "      <td>0.051564</td>\n",
              "      <td>0.002515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4456059202748560</td>\n",
              "      <td>2020请对我好点，第一天就感冒发烧…………?</td>\n",
              "      <td>0.764661</td>\n",
              "      <td>0.221836</td>\n",
              "      <td>0.013503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4456071538097160</td>\n",
              "      <td>2020的第一天是这样的感冒发烧流鼻涕2厦门?</td>\n",
              "      <td>0.817297</td>\n",
              "      <td>0.178157</td>\n",
              "      <td>0.004546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4456071366149900</td>\n",
              "      <td>我发烧我胃痛你在乎吗我通宵我失踪你在乎吗我难过我不安你在乎吗我委屈我心痛你在乎吗，我爱你你在乎?</td>\n",
              "      <td>0.826483</td>\n",
              "      <td>0.150577</td>\n",
              "      <td>0.022940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4456070334552490</td>\n",
              "      <td>20.1.1发烧，一个晚上没睡好，早上起来就去诊所看了一下，晚上写作业，明天又有考试?</td>\n",
              "      <td>0.793562</td>\n",
              "      <td>0.201865</td>\n",
              "      <td>0.004573</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               微博id  ...         1\n",
              "2  4456068145061890  ...  0.002515\n",
              "5  4456059202748560  ...  0.013503\n",
              "7  4456071538097160  ...  0.004546\n",
              "8  4456071366149900  ...  0.022940\n",
              "9  4456070334552490  ...  0.004573\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "539jKuIS532X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_cat = np.argmax(df_good[['-1','0','1']].values, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvQBu4Z67EWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8fba49cc-eb8d-41be-d33d-ab26a0a3584f"
      },
      "source": [
        "num_total = len(pred_cat)\n",
        "print('-1 ration', (pred_cat==0).sum()/num_total)\n",
        "print('0 ration', (pred_cat==1).sum()/num_total)\n",
        "print('1 ration', (pred_cat==2).sum()/num_total)\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1 ration 0.15889685546492033\n",
            "0 ration 0.5935058007917213\n",
            "1 ration 0.24759734374335832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMJZCxpY7a4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}